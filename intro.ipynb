{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from groq import Groq\n",
    "load_dotenv()\n",
    " \n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm good!\n"
     ]
    }
   ],
   "source": [
    "# Set the system prompt\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\":\n",
    "    \"You are a helpful assistant. You reply with very short answers.\"\n",
    "}\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = [system_prompt]\n",
    "\n",
    "\n",
    "# Get user input from the console\n",
    "user_input = 'How are you'\n",
    "\n",
    "  # Append the user input to the chat history\n",
    "chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "response = client.chat.completions.create(model=\"llama3-70b-8192\",\n",
    "                                            messages=chat_history,\n",
    "                                            max_tokens=100,\n",
    "                                            temperature=1.2)\n",
    "  # Append the response to the chat history\n",
    "chat_history.append({\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": response.choices[0].message.content\n",
    "  })\n",
    "  # Print the response\n",
    "print(\"Assistant:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/27 04:20:33 INFO mlflow.tracking.fluent: Experiment with name 'Places' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Capital of India, rich history, cultural hub, famous for cuisine, monuments like Red Fort & Qutub Minar.\n",
      "üèÉ View run unequaled-koi-90 at: http://127.0.0.1:5000/#/experiments/303638892987493777/runs/4cd9588fe2ed478eaa06a566b4911a02\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/303638892987493777\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "# Set the system prompt\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\":\n",
    "    \"You are a helpful assistant. You reply with very short answers.\"\n",
    "}\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = [system_prompt]\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"Places\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get user input from the console\n",
    "user_input = 'Tell me about Delhi'\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('user_input',user_input) \n",
    "    # Append the user input to the chat history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(model=\"llama3-70b-8192\",\n",
    "                                                messages=chat_history,\n",
    "                                                max_tokens=100,\n",
    "                                                temperature=1.2)\n",
    "    # Append the response to the chat history\n",
    "    chat_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.choices[0].message.content\n",
    "    })\n",
    "    mlflow.log_param('AI output',response.choices[0].message.content) \n",
    "    # Print the response\n",
    "    print(\"Assistant:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:  \n",
    "    def __init__(self):  \n",
    "        self.memory = []  # Conversation history  \n",
    "        self.tools = {} \n",
    "        self.user_input = None   # Registered tools  \n",
    "\n",
    "    def register_tool(self, tool_name, tool_function):  \n",
    "        self.tools[tool_name] = tool_function  \n",
    "\n",
    "    def respond(self, user_input) -> str:  \n",
    "            self.user_input = user_input\n",
    "            # Check if Groq AI tool is registered and use it\n",
    "            if \"groq_ai\" in self.tools:\n",
    "                groq_response = self.tools[\"groq_ai\"](self.user_input)\n",
    "                return groq_response\n",
    "            return f\"I'm a simple agent. You said: {self.user_input}. How can I help?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(self, user_input):  \n",
    "    self.memory.append({\"user\": user_input})  \n",
    "\n",
    "    if \"weather\" in user_input:  \n",
    "        return \"I can check the weather. Please say your city.\"  \n",
    "    else:  \n",
    "        return \"Sorry, I didn't understand that.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List  \n",
    "\n",
    "class MemoryManager:  \n",
    "    def __init__(self):  \n",
    "        self.short_term: List[Dict] = []  # Chat history  \n",
    "        self.long_term: Dict = {}         # User preferences  \n",
    "\n",
    "    def update_short_term(self, role: str, content: str):  \n",
    "        self.short_term.append({\"role\": role, \"content\": content})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re\n",
    "import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "\n",
    "\n",
    "def weather_tool() -> str:  \n",
    "    \"\"\"Fetches weather for a Lat Long.  Returns: str.\"\"\"  \n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "    # Make sure all required weather variables are listed here\n",
    "    # The order of variables in hourly or daily is important to assign them correctly below\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": 28.27,\n",
    "        \"longitude\": 76.14,\n",
    "        \"hourly\": \"temperature_2m\"\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "    # Process first location. Add a for-loop for multiple locations or weather models\n",
    "    response = responses[0]\n",
    "    print(f\"Coordinates {response.Latitude()}¬∞N {response.Longitude()}¬∞E\")\n",
    "    print(f\"Elevation {response.Elevation()} m asl\")\n",
    "    print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "    print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "    # Process hourly data. The order of variables needs to be the same as requested.\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "    hourly_data = {\"date\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    )}\n",
    "\n",
    "    hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "\n",
    "    hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "    return hourly_dataframe.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 28.25¬∞N 76.125¬∞E\n",
      "Elevation 269.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "                       date  temperature_2m\n",
      "0 2025-02-26 00:00:00+00:00         17.5455\n"
     ]
    }
   ],
   "source": [
    "response = weather_tool()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent()  \n",
    "agent.register_tool(\"get_weather\", weather_tool)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(self, user_input):  \n",
    "    if \"weather\" in user_input:  \n",
    "        weather = self.tools[\"get_weather\"]\n",
    "        return weather  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run stately-fly-400 at: http://127.0.0.1:5000/#/experiments/303638892987493777/runs/5b7f4fd52c734ac49506a4958202a9ed\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/303638892987493777\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():  \n",
    "    mlflow.log_param(\"tool\", \"get_weather\")  \n",
    "    mlflow.log_metric(\"response_time_ms\", 320)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_groq_ai(query: str) -> str:\n",
    "    # Append the user input to the chat history\n",
    "    # Set the system prompt\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\":\n",
    "        \"You are a helpful assistant. You reply with very short answers.\"\n",
    "    }\n",
    "\n",
    "    # Initialize the chat history\n",
    "    chat_history = [system_prompt]\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    # Append the user input to the chat history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    response = client.chat.completions.create(model=\"llama3-70b-8192\",\n",
    "                                                messages=chat_history,\n",
    "                                                max_tokens=100,\n",
    "                                                temperature=1.2)\n",
    "    # Append the response to the chat history\n",
    "    chat_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.choices[0].message.content\n",
    "    })\n",
    "    # Print the response\n",
    "    return (\"Assistant:\", response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Any, Dict, Optional, List, get_type_hints\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import urllib.request\n",
    "import json\n",
    "import mlflow\n",
    "from groq import Groq\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import _GenericAlias\n",
    "import pytz  # For time zone tool\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"AI Agent Toolkit\")\n",
    "\n",
    "@dataclass\n",
    "class Tool:\n",
    "    \"\"\"A callable tool with metadata for the agent\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    func: Callable[..., str]\n",
    "    parameters: Dict[str, Dict[str, str]]\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> str:\n",
    "        \"\"\"Execute the tool with MLflow logging\"\"\"\n",
    "        with mlflow.start_run(nested=True):\n",
    "            try:\n",
    "                # Log tool parameters\n",
    "                mlflow.log_params(kwargs)\n",
    "                start_time = datetime.now()\n",
    "                \n",
    "                # Execute tool\n",
    "                result = self.func(*args, **kwargs)\n",
    "                \n",
    "                # Log metrics\n",
    "                execution_time = (datetime.now() - start_time).total_seconds()\n",
    "                mlflow.log_metric(\"execution_time\", execution_time)\n",
    "                mlflow.log_text(result, \"tool_output.txt\")\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                mlflow.log_param(\"error\", str(e))\n",
    "                return f\"Tool error: {str(e)}\"\n",
    "\n",
    "def parse_docstring_params(docstring: str) -> Dict[str, str]:\n",
    "    \"\"\"Parse parameter descriptions from docstring\"\"\"\n",
    "    params = {}\n",
    "    if not docstring:\n",
    "        return params\n",
    "    \n",
    "    current_param = None\n",
    "    for line in docstring.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Parameters:\"):\n",
    "            current_param = \"parameters\"\n",
    "        elif line.startswith(\"- \"):\n",
    "            parts = line[2:].split(\":\", 1)\n",
    "            if len(parts) == 2:\n",
    "                current_param = parts[0].strip()\n",
    "                params[current_param] = parts[1].strip()\n",
    "        elif current_param and line:\n",
    "            params[current_param] += \" \" + line.strip()\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_type_description(type_hint: Any) -> str:\n",
    "    \"\"\"Convert type hints to human-readable format\"\"\"\n",
    "    if isinstance(type_hint, _GenericAlias):\n",
    "        args = \", \".join(map(get_type_description, type_hint.__args__))\n",
    "        return f\"{type_hint._name}[{args}]\"\n",
    "    return getattr(type_hint, \"__name__\", str(type_hint))\n",
    "\n",
    "def tool(name: str = None, requires_auth: bool = False):\n",
    "    \"\"\"Decorator to create Tool instances from functions\"\"\"\n",
    "    def decorator(func: Callable[..., str]) -> Tool:\n",
    "        nonlocal name\n",
    "        tool_name = name or func.__name__\n",
    "        doc = inspect.getdoc(func) or \"\"\n",
    "        \n",
    "        # Parse parameters\n",
    "        param_docs = parse_docstring_params(doc)\n",
    "        type_hints = get_type_hints(func)\n",
    "        sig = inspect.signature(func)\n",
    "        \n",
    "        parameters = {}\n",
    "        for param in sig.parameters.values():\n",
    "            if param.name == \"self\":\n",
    "                continue\n",
    "                \n",
    "            parameters[param.name] = {\n",
    "                \"type\": get_type_description(type_hints.get(param.name, Any)),\n",
    "                \"description\": param_docs.get(param.name, \"No description\"),\n",
    "                \"required\": param.default == inspect.Parameter.empty\n",
    "            }\n",
    "        \n",
    "        # Add auth requirement to description\n",
    "        if requires_auth:\n",
    "            doc += \"\\n\\nRequires authentication: Yes\"\n",
    "        \n",
    "        return Tool(\n",
    "            name=tool_name,\n",
    "            description=doc.split(\"\\n\\n\")[0],\n",
    "            func=func,\n",
    "            parameters=parameters\n",
    "        )\n",
    "    return decorator\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Core AI agent with tool integration and planning capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "        self.tools: Dict[str, Tool] = {}\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def add_tool(self, tool: Tool) -> None:\n",
    "        \"\"\"Register a tool with the agent\"\"\"\n",
    "        self.tools[tool.name] = tool\n",
    "        logger.info(f\"Registered tool: {tool.name}\")\n",
    "        \n",
    "    def create_system_prompt(self) -> str:\n",
    "        \"\"\"Generate the system prompt with current tool definitions\"\"\"\n",
    "        tools_schema = []\n",
    "        for tool in self.tools.values():\n",
    "            tool_schema = {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": {\n",
    "                    param: {\n",
    "                        \"type\": info[\"type\"],\n",
    "                        \"description\": info[\"description\"],\n",
    "                        \"required\": info[\"required\"]\n",
    "                    }\n",
    "                    for param, info in tool.parameters.items()\n",
    "                }\n",
    "            }\n",
    "            tools_schema.append(tool_schema)\n",
    "        \n",
    "        return f\"\"\"You are an AI assistant with access to tools. Your responsibilities:\n",
    "1. Analyze user queries to determine if tools are needed\n",
    "2. Select appropriate tools and parameters\n",
    "3. Return JSON with tool calls or direct response\n",
    "\n",
    "Available tools: {json.dumps(tools_schema, indent=2)}\n",
    "\n",
    "Response format:\n",
    "{{\n",
    "    \"requires_tool\": boolean,\n",
    "    \"response\": string (optional),\n",
    "    \"tools\": [\n",
    "        {{\n",
    "            \"name\": string,\n",
    "            \"parameters\": object\n",
    "        }}\n",
    "    ]\n",
    "}}\"\"\"\n",
    "\n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        \"\"\"Use LLM to determine required actions\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.create_system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Failed to parse LLM response: {e}\")\n",
    "            return {\"error\": \"Failed to process query\"}\n",
    "\n",
    "    def execute(self, query: str) -> str:\n",
    "        \"\"\"Full execution flow with MLflow tracking\"\"\"\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_param(\"query\", query)\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Store conversation history\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "            \n",
    "            # Get LLM plan\n",
    "            plan = self.process_query(query)\n",
    "            if \"error\" in plan:\n",
    "                return \"Failed to process request\"\n",
    "            \n",
    "            mlflow.log_dict(plan, \"plan.json\")\n",
    "            \n",
    "            if not plan.get(\"requires_tool\", False):\n",
    "                mlflow.log_metric(\"direct_response\", 1)\n",
    "                return plan.get(\"response\", \"No response generated\")\n",
    "            \n",
    "            # Execute tools\n",
    "            results = []\n",
    "            for tool_call in plan.get(\"tools\", []):\n",
    "                tool_name = tool_call[\"name\"]\n",
    "                if tool_name not in self.tools:\n",
    "                    logger.warning(f\"Tool {tool_name} not found\")\n",
    "                    continue\n",
    "                    \n",
    "                tool = self.tools[tool_name]\n",
    "                try:\n",
    "                    result = tool(**tool_call[\"parameters\"])\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    results.append(f\"Error with {tool_name}: {str(e)}\")\n",
    "            \n",
    "            # Log metrics\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            mlflow.log_metrics({\n",
    "                \"total_time\": execution_time,\n",
    "                \"tools_used\": len(results)\n",
    "            })\n",
    "            \n",
    "            return \"\\n\".join(results)\n",
    "\n",
    "# ======================\n",
    "# Tool Implementations\n",
    "# ======================\n",
    "\n",
    "@tool(\"currency_converter\", requires_auth=True)\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Convert between currencies using real exchange rates\n",
    "    \n",
    "    Parameters:\n",
    "        - amount: The amount to convert\n",
    "        - from_currency: 3-letter currency code (e.g. USD)\n",
    "        - to_currency: Target currency code\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"CURRENCY_API_KEY\")\n",
    "    url = f\"https://api.exchangerate.host/convert?from={from_currency}&to={to_currency}&amount={amount}\"\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = json.load(response)\n",
    "            if data.get(\"success\"):\n",
    "                return f\"{amount} {from_currency} = {data['result']:.2f} {to_currency}\"\n",
    "            return \"Conversion failed\"\n",
    "    except Exception as e:\n",
    "        return f\"API error: {str(e)}\"\n",
    "\n",
    "@tool(\"time_converter\")\n",
    "def convert_time(time_str: str, from_tz: str, to_tz: str) -> str:\n",
    "    \"\"\"Convert time between time zones\n",
    "    \n",
    "    Parameters:\n",
    "        - time_str: Time in HH:MM format\n",
    "        - from_tz: Source timezone (e.g. Europe/Paris)\n",
    "        - to_tz: Target timezone (e.g. America/New_York)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time = datetime.strptime(time_str, \"%H:%M\")\n",
    "        from_zone = pytz.timezone(from_tz)\n",
    "        to_zone = pytz.timezone(to_tz)\n",
    "        \n",
    "        localized = from_zone.localize(time)\n",
    "        converted = localized.astimezone(to_zone)\n",
    "        return converted.strftime(\"%H:%M %Z\")\n",
    "    except Exception as e:\n",
    "        return f\"Time conversion error: {str(e)}\"\n",
    "\n",
    "@tool(\"weather_lookup\", requires_auth=True)\n",
    "def get_weather(city: str, country: str) -> str:\n",
    "    \"\"\"Get current weather conditions\n",
    "    \n",
    "    Parameters:\n",
    "        - city: City name\n",
    "        - country: 2-letter country code\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"WEATHER_API_KEY\")\n",
    "    url = f\"http://api.weatherapi.com/v1/current.json?key={api_key}&q={city},{country}\"\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = json.load(response)\n",
    "            current = data[\"current\"]\n",
    "            return f\"{current['temp_c']}¬∞C, {current['condition']['text']}\"\n",
    "    except Exception as e:\n",
    "        return f\"Weather API error: {str(e)}\"\n",
    "\n",
    "# ======================\n",
    "# Usage Example\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize agent\n",
    "    agent = Agent()\n",
    "    \n",
    "    # Register tools\n",
    "    agent.add_tool(convert_currency)\n",
    "    agent.add_tool(convert_time)\n",
    "    agent.add_tool(get_weather)\n",
    "    \n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"Convert 100 USD to EUR\",\n",
    "        \"What time is 14:30 Europe/Paris in America/New_York?\",\n",
    "        \"What's the weather in Tokyo, JP?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        response = agent.execute(query)\n",
    "        print(f\"Response: {response}\")\n",
    "    \n",
    "    print(\"\\nMLflow Experiment Tracking:\")\n",
    "    print(f\"Track results at: {mlflow.get_tracking_uri()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
